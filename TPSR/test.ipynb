{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import sympy as sp\n",
    "from parsers import get_parser\n",
    "\n",
    "import symbolicregression\n",
    "from symbolicregression.envs import build_env\n",
    "from symbolicregression.model import build_modules\n",
    "from symbolicregression.trainer import Trainer\n",
    "from symbolicregression.e2e_model import Transformer, pred_for_sample_no_refine, respond_to_batch , pred_for_sample, refine_for_sample, pred_for_sample_test, refine_for_sample_test \n",
    "from dyna_gym.agents.uct import UCT\n",
    "from dyna_gym.agents.mcts import update_root, convert_to_json, print_tree\n",
    "from rl_env import RLEnv\n",
    "from default_pi import E2EHeuristic, NesymresHeuristic\n",
    "from symbolicregression.metrics import compute_metrics\n",
    "\n",
    "\n",
    "from nesymres.src.nesymres.architectures.model import Model\n",
    "from nesymres.utils import load_metadata_hdf5\n",
    "from nesymres.dclasses import FitParams, NNEquation, BFGSParams\n",
    "from functools import partial\n",
    "from sympy import lambdify\n",
    "from reward import compute_reward_e2e, compute_reward_nesymres\n",
    "import omegaconf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "params = Namespace(backbone_model='e2e', seed=23, width=3, horizon=200, rollout=3, num_beams=1, train_value=False, no_seq_cache=True, no_prefix_cache=True, sample_only=False, ucb_constant=1.0, ucb_base=10.0, uct_alg='uct', ts_mode='best', alg='mcts', entropy_weighted_strategy='none', dump_path='', refinements_types='method=BFGS_batchsize=256_metric=/_mse', eval_dump_path=None, save_results=True, exp_name='debug', print_freq=100, save_periodic=25, exp_id='', fp16=False, amp=-1, rescale=True, embedder_type='LinearPoint', emb_emb_dim=64, enc_emb_dim=512, dec_emb_dim=512, n_emb_layers=1, n_enc_layers=2, n_dec_layers=16, n_enc_heads=16, n_dec_heads=16, emb_expansion_factor=1, n_enc_hidden_layers=1, n_dec_hidden_layers=1, norm_attention=False, dropout=0, attention_dropout=0, share_inout_emb=True, enc_positional_embeddings=None, dec_positional_embeddings='learnable', env_base_seed=0, test_env_seed=1, batch_size=1, batch_size_eval=64, optimizer='adam_inverse_sqrt,warmup_updates=10000', lr=1e-05, clip_grad_norm=0.5, n_steps_per_epoch=3000, max_epoch=100000, stopping_criterion='', accumulate_gradients=1, num_workers=10, train_noise_gamma=0.0, ablation_to_keep=None, max_input_points=200, n_trees_to_refine=10, export_data=False, reload_data='', reload_size=-1, batch_load=False, env_name='functions', queue_strategy=None, collate_queue_size=2000, use_sympy=False, simplify=False, use_abs=False, operators_to_downsample='div_0,arcsin_0,arccos_0,tan_0.2,arctan_0.2,sqrt_5,pow2_3,inv_3', operators_to_not_repeat='', max_unary_depth=6, required_operators='', extra_unary_operators='', extra_binary_operators='', extra_constants=None, min_input_dimension=1, max_input_dimension=10, min_output_dimension=1, max_output_dimension=1, enforce_dim=True, use_controller=True, float_precision=3, mantissa_len=1, max_exponent=100, max_exponent_prefactor=1, max_token_len=0, tokens_per_batch=10000, pad_to_max_dim=True, max_int=10, min_binary_ops_per_dim=0, max_binary_ops_per_dim=1, max_binary_ops_offset=4, min_unary_ops=0, max_unary_ops=4, min_op_prob=0.01, max_len=200, min_len_per_dim=5, max_centroids=10, prob_const=0.0, reduce_num_constants=True, use_skeleton=False, prob_rand=0.0, max_trials=1, n_prediction_points=200, tasks='functions', beam_eval=True, max_generated_output_len=200, beam_eval_train=0, beam_size=1, beam_type='sampling', beam_temperature=0.1, beam_length_penalty=1, lam=0.1, beam_early_stopping=True, beam_selection_metrics=1, max_number_bags=10, reload_model='', reload_checkpoint='', validation_metrics='r2_zero,r2,accuracy_l1_biggio,accuracy_l1_1e-3,accuracy_l1_1e-2,accuracy_l1_1e-1,_complexity', debug_train_statistics=False, eval_noise_gamma=0.0, eval_size=10000, eval_noise_type='additive', eval_noise=0, eval_only=False, eval_from_exp='', eval_data='', eval_verbose=0, eval_verbose_print=False, eval_input_length_modulo=-1, eval_on_pmlb=False, eval_mcts_on_pmlb=False, eval_in_domain=False, eval_mcts_in_domain=False, debug_slurm=False, debug=True, cpu=False, local_rank=-1, gpu_to_use='0', master_port=-1, windows=False, nvidia_apex=False, max_src_len=200, max_target_len=200, reward_type='nmse', reward_coef=1, vf_coef=0.0001, target_kl=1, entropy_coef=0.01, kl_regularizer=0.001, warmup_epoch=5, lr_patience=100, save_model=True, save_eval_dic='./eval_result', update_modules='all', actor_lr=1e-06, critic_lr=1e-05, kl_coef=0.01, rl_alg='ppo', run_id=1, pmlb_data_type='feynman', target_noise=0.0, prediction_sigmas='1,2,4,8,16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "np.random.seed(params.seed)\n",
    "torch.manual_seed(params.seed)\n",
    "torch.cuda.manual_seed(params.seed)\n",
    "params.debug = True\n",
    "params.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint path does not exist, checkpoint.pth\n"
     ]
    }
   ],
   "source": [
    "equation_env = build_env(params)\n",
    "modules = build_modules(equation_env, params)\n",
    "if not params.cpu:\n",
    "    assert torch.cuda.is_available()\n",
    "symbolicregression.utils.CUDA = not params.cpu\n",
    "trainer = Trainer(modules, equation_env, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Transformer.__init__() missing 1 required positional argument: 'samples'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_172844/512748272.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mequation_env\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Transformer.__init__() missing 1 required positional argument: 'samples'"
     ]
    }
   ],
   "source": [
    "model = Transformer(params = params, env=equation_env)\n",
    "model.to(params.device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_172844/1013511770.py\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m          \u001b[0;31m# Your model trainer instance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m          \u001b[0;31m# Model parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m           \u001b[0;31m# Your trained model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mtarget_noise\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m    \u001b[0;31m# Amount of noise to add to target values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m29910\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Random seed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "from evaluate import evaluate_pmlb\n",
    "\n",
    "# Basic usage\n",
    "scores = evaluate_pmlb(\n",
    "    trainer,          # Your model trainer instance\n",
    "    params,          # Model parameters\n",
    "    model,           # Your trained model\n",
    "    target_noise=0.0,    # Amount of noise to add to target values\n",
    "    random_state=29910,  # Random seed\n",
    "    verbose=False,       # Whether to show detailed output\n",
    "    save=True,          # Whether to save results\n",
    "    save_suffix=\"./eval_result/my_evaluation.csv\"  # Where to save results\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TPSR",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
